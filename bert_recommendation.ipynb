{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Content Based Recommendation Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tha.jpg)\n",
    "<center>(Don't mind me, the editor is just having fun)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is Google's open source NLP technique, it stands for Bidirectional Encoder Representations from Transformers. What this means is that it becvomes very easy to create vector representations of words and sentences. A vector representation is a way of saying we can have a numeric representation of a word or sentence which can be fed to a machine learning pipeline and do \"stuff\" to them. (Stuff is a adequate word, look it up). \n",
    "\n",
    "One of the most important steps in a Data Science / ML pipeline is feature engineering, BERT is a great starting point.\n",
    "\n",
    "Why is BERT different to other better known word embedding techniques?\n",
    "\n",
    ">*\"Why does this matter? Pre-trained representations can either be context-free or contextual, and contextual representations can further be unidirectional or bidirectional. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. For example, the word “bank” would have the same context-free representation in “bank account” and “bank of the river.” Contextual models instead generate a representation of each word that is based on the other words in the sentence. For example, in the sentence “I accessed the bank account,” a unidirectional contextual model would represent “bank” based on “I accessed the” but not “account.” However, BERT represents “bank” using both its previous and next context — “I accessed the ... account” — starting from the very bottom of a deep neural network, making it deeply bidirectional.\" [Link](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok lets say we have BERT, then many interesting projects come to mind.\n",
    "- Text classification \n",
    "- Text summarization\n",
    "- Sentiment analysis\n",
    "- **Recommendation engine** you knew I was going to choose this one, you are smart, I like you.\n",
    "\n",
    "So that based on some text we can recommend other similar texts. So lets say you read Herman Melville's Moby Dick, you might be interested in some aquatic adventures! Perhaps recommending Jules Verne's Twenty Thousand Leagues Under the Sea, or The old man and the sea by Ernest Hemingway or...\n",
    "\n",
    "![](aquaman.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting everything up\n",
    "\n",
    "Pretty much I followed [this tutorial](https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/) to set up BERT's client and server in my computer. So you know you can go there and follow it or stay here, I have better images though. It is important for this to be fast that you have a cool graphics card, I have a GTX 1080 that I use exclusively for Deep Learning and never for games... never.\n",
    "\n",
    "You will also need to download the pretrained models, I donwloaded a couple a one uncased and the other cased here are the links:\n",
    "- [uncased_L-12_H-768_A-12](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip)\n",
    "- [cased_L-24_H-1024_A-16](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip)\n",
    "\n",
    "You might run into trouble if you want to use your gpu to train models if you haven't before, here is an excelent tutorial on how to set up most stuff for running ML on GPU on Windows 10 [Link](https://harangdev.github.io/tips/1/), create a virtual environment and stuff everything in there.\n",
    "\n",
    "Ok so you have everything running and ready, you will see the following on your CLI.\n",
    "\n",
    "> *bert-serving-start -model_dir uncased_L-12_H-768_A-12 -gpu_memory_fraction 0.75 -cors 1 -num_worker 1 -device_map 0 -max_seq_len 40*\n",
    "> *bert-serving-start -model_dir cased_L-24_H-1024_A-16 -gpu_memory_fraction 0.75 -cors 1 -num_worker 1 -device_map 0 -max_seq_len 40*\n",
    "\n",
    "![](server_start.PNG)\n",
    "\n",
    "A quick explanation on what the parameters mean:\n",
    "- **-model_dir** : the path to where you downloaded the pretrained models\n",
    "- **-gpu_memory_fraction** : the proportion of memory from your gpu that you will allocate to this process\n",
    "- **-cors** : Number of concurrent connections (so how many clients will be connected at one time to the server)\n",
    "- **-device_map** : the id from the gpus you will use, I have one and it is the 0\n",
    "- **-max_seq_len** : In order to create the vector from a sentence or sequence of tokens, there is some transformations I haven't read in detail, but basically this parameter identifies how many tokens or words will be used to create the vector that will identify the sentence.\n",
    "\n",
    "Following are the rest of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "           ckpt_name = bert_model.ckpt\n",
    "         config_name = bert_config.json\n",
    "                cors = 1\n",
    "                 cpu = False\n",
    "          device_map = [0]\n",
    "       do_lower_case = True\n",
    "  fixed_embed_length = False\n",
    "                fp16 = False\n",
    " gpu_memory_fraction = 0.9\n",
    "       graph_tmp_dir = None\n",
    "    http_max_connect = 10\n",
    "           http_port = None\n",
    "        mask_cls_sep = False\n",
    "      max_batch_size = 256\n",
    "         max_seq_len = 25\n",
    "           model_dir = cased_L-24_H-1024_A-16\n",
    "    no_special_token = False\n",
    "          num_worker = 1\n",
    "       pooling_layer = [-2]\n",
    "    pooling_strategy = REDUCE_MEAN\n",
    "                port = 5555\n",
    "            port_out = 5556\n",
    "       prefetch_size = 10\n",
    " priority_batch_size = 16\n",
    "show_tokens_to_client = False\n",
    "     tuned_model_dir = None\n",
    "             verbose = False\n",
    "                 xla = False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find data, use BERT server, create vectors\n",
    "\n",
    "So first step is to read the data that we will use for this recommendation engine, it should be some text or group of texts that will need to undergo (probably) some cleaning before sending it to encoding.\n",
    "\n",
    "On the tutorial I found they had a link to a Twitter sentiment competition which I used, but I want to see how this works with other text, and I found this. Hope this works since I am doing one day before I am presenting it.\n",
    "\n",
    "I will be using this data [Link](https://github.com/groveco/content-engine/blob/master/sample-data.csv) which I found while browsing this [Link](http://blog.untrod.com/2016/06/simple-similar-products-recommendation-engine-in-python.html). They use Tf-Idf to create the feature vectors, but that is so 2016..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from bert_serving.client import BertClient\n",
    "from vsm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv('sample-data.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Active classic boxers - There's a reason why o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Active sport boxer briefs - Skinning up Glory ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Active sport briefs - These superbreathable no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Alpine guide pants - Skin in, climb ice, switc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Alpine wind jkt - On high ridges, steep ice an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        description\n",
       "0   1  Active classic boxers - There's a reason why o...\n",
       "1   2  Active sport boxer briefs - Skinning up Glory ...\n",
       "2   3  Active sport briefs - These superbreathable no...\n",
       "3   4  Alpine guide pants - Skin in, climb ice, switc...\n",
       "4   5  Alpine wind jkt - On high ridges, steep ice an..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some functions to clean the text before sending it to encoding, basically what it does is to leaves only alphabetic characters, removes unicode characters if present, removes extra spaces and changes all caps if present to Title case. I left the words cases and I will be using the cased pretrained models, I could change everthing to lowercase and use the other one, part of the thrill of reasearch that I am not doing just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_case(word: str) -> str:\n",
    "    if len(word) == 0:\n",
    "        return word\n",
    "    elif word[0].isupper():\n",
    "        return word.capitalize()\n",
    "    else:\n",
    "        return word.lower()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    text = text.strip().split(' ')\n",
    "    text = ' '.join([proper_case(word) for word in text if len(word) > 0])\n",
    "       \n",
    "    return text\n",
    "\n",
    "text['clean_text'] = text.description.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Active classic boxers - There's a reason why o...</td>\n",
       "      <td>Active classic boxers There's a reason why our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Active sport boxer briefs - Skinning up Glory ...</td>\n",
       "      <td>Active sport boxer briefs Skinning up Glory re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Active sport briefs - These superbreathable no...</td>\n",
       "      <td>Active sport briefs These superbreathable no f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Alpine guide pants - Skin in, climb ice, switc...</td>\n",
       "      <td>Alpine guide pants Skin in climb ice switch to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Alpine wind jkt - On high ridges, steep ice an...</td>\n",
       "      <td>Alpine wind jkt On high ridges steep ice and a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Active classic boxers - There's a reason why o...   \n",
       "1  Active sport boxer briefs - Skinning up Glory ...   \n",
       "2  Active sport briefs - These superbreathable no...   \n",
       "3  Alpine guide pants - Skin in, climb ice, switc...   \n",
       "4  Alpine wind jkt - On high ridges, steep ice an...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  Active classic boxers There's a reason why our...  \n",
       "1  Active sport boxer briefs Skinning up Glory re...  \n",
       "2  Active sport briefs These superbreathable no f...  \n",
       "3  Alpine guide pants Skin in climb ice switch to...  \n",
       "4  Alpine wind jkt On high ridges steep ice and a...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets encode our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1024)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc = BertClient()\n",
    "clean_encoded_text = bc.encode(text.clean_text.tolist())\n",
    "clean_encoded_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now out feature vectors, numerical representations of the text we submitted. One issue we see from looking at the shape of the dataset is that we have many more columns than we have rows. Since we don't have more data (always our best ally) we might resort to some dimensionionality reduction tecniques.\n",
    "\n",
    "Lets try PCA here:\n",
    "\n",
    "> Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables. [wiki](https://en.wikipedia.org/wiki/Principal_component_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 128\n",
    "pca = PCA(n_components=n, whiten=False, random_state=0)\n",
    "pca_text = pca.fit_transform(clean_encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how much of the total variance is being explained by the transformation. Since we reducing the dimensionality, we should expect some loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss from 1024 to 128 = 12.06%\n"
     ]
    }
   ],
   "source": [
    "print('Loss from 1024 to {} = {:.2f}%'.format(n, 100*(1-np.sum(pca.explained_variance_ratio_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets say that is good enough (again this should be tested and researched). To create the recommendation engine, we want to find a way to measure the distance between 2 vectors, and there are plenty of ways to do so.\n",
    "\n",
    "There is a library I am developing with tools for Natural Language Understanding so I will used those, the code is on the same folder on vsm.py\n",
    "\n",
    "Once we select our metric, it is our task to find the vectors that are closest to the one we selected, that can be very slow when we have many vectors because we need to compare each vector against each other in the dataset.\n",
    "\n",
    "So depending on the size if we wanted to have the recommendation for each vector, we would end up with a triangular matrix. Time complexity say for using euclidean distance would be O(m*n^2) where n is the number of vectors and m the size of them.\n",
    "\n",
    "Anyway..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recommendation(object):\n",
    "    def __init__(self, text: pd.DataFrame, col: str):\n",
    "        self.text = text\n",
    "        self.col = col\n",
    "        self.clean_text = None\n",
    "        self.encoded_text = None\n",
    "        self.encoded_pca = None\n",
    "        self.bc = BertClient()\n",
    "    \n",
    "    \n",
    "    def __clean_text(self):\n",
    "        self.clean_text = self.text[self.col].apply(clean_text).tolist()\n",
    "    \n",
    "    \n",
    "    def encode(self, n_components=1, random_state=0, use_pca=False):\n",
    "        self.use_pca = use_pca\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        if self.clean_text is None:\n",
    "            self.__clean_text()\n",
    "        \n",
    "        if self.encoded_text is None:\n",
    "            self.encoded_text = self.bc.encode(self.clean_text)\n",
    "            \n",
    "        if self.use_pca:\n",
    "            self.pca = PCA(n_components=self.n_components, random_state=self.random_state)\n",
    "            self.pca.fit(self.encoded_text)\n",
    "            self.encoded_pca =  self.pca.transform(self.encoded_text)\n",
    "        \n",
    "    \n",
    "    def recommend(self, text: str, distance=cosine_distance, n_recommend=10, ascending=True):\n",
    "        enc_text = self.bc.encode([clean_text(text)])\n",
    "        \n",
    "        if self.use_pca:\n",
    "            enc_pca = self.pca.transform(enc_text)\n",
    "            self.text[distance.__name__] = np.apply_along_axis(distance, 1, self.encoded_pca, enc_pca)\n",
    "            \n",
    "        else:\n",
    "            self.text[distance.__name__] = np.apply_along_axis(distance, 1, self.encoded_text, enc_text)\n",
    "        \n",
    "        return self.text.sort_values(by=distance.__name__, ascending=ascending).head(n_recommend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = recommendation(text, 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sors_\\conda-gpu\\lib\\site-packages\\bert_serving\\client\\__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=40\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    }
   ],
   "source": [
    "rec.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>cosine_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Active classic boxers - There's a reason why o...</td>\n",
       "      <td>Active classic boxers There's a reason why our...</td>\n",
       "      <td>5.960464e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>Continental shorts - Wrinkle-resistant travel ...</td>\n",
       "      <td>Continental shorts Wrinkle resistant travel sh...</td>\n",
       "      <td>3.168947e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>Cap 3 bottoms - The unwavering foundation for ...</td>\n",
       "      <td>Cap bottoms The unwavering foundation for any ...</td>\n",
       "      <td>3.196377e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>Retro grade shorts - As advantageous as a numb...</td>\n",
       "      <td>Retro grade shorts As advantageous as a number...</td>\n",
       "      <td>3.285807e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>Active boxer briefs - A no-fuss travel compani...</td>\n",
       "      <td>Active boxer briefs A no fuss travel companion...</td>\n",
       "      <td>3.371626e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>Custodian pants - short - The graveyard shift ...</td>\n",
       "      <td>Custodian pants short The graveyard shift has ...</td>\n",
       "      <td>3.381079e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>Duck pants - reg - Essential wear for splittin...</td>\n",
       "      <td>Duck pants reg Essential wear for splitting lo...</td>\n",
       "      <td>3.393221e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Baggies shorts - Even Baggies, our most popula...</td>\n",
       "      <td>Baggies shorts Even Baggies our most popular s...</td>\n",
       "      <td>3.402412e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>Stretch polo - Core to the nomadic lifestyle, ...</td>\n",
       "      <td>Stretch polo Core to the nomadic lifestyle our...</td>\n",
       "      <td>3.427577e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>El cap jkt - Resistant to hard play but irresi...</td>\n",
       "      <td>El cap jkt Resistant to hard play but irresist...</td>\n",
       "      <td>3.459531e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           description  \\\n",
       "0    Active classic boxers - There's a reason why o...   \n",
       "28   Continental shorts - Wrinkle-resistant travel ...   \n",
       "439  Cap 3 bottoms - The unwavering foundation for ...   \n",
       "399  Retro grade shorts - As advantageous as a numb...   \n",
       "493  Active boxer briefs - A no-fuss travel compani...   \n",
       "462  Custodian pants - short - The graveyard shift ...   \n",
       "480  Duck pants - reg - Essential wear for splittin...   \n",
       "11   Baggies shorts - Even Baggies, our most popula...   \n",
       "131  Stretch polo - Core to the nomadic lifestyle, ...   \n",
       "61   El cap jkt - Resistant to hard play but irresi...   \n",
       "\n",
       "                                            clean_text  cosine_distance  \n",
       "0    Active classic boxers There's a reason why our...     5.960464e-08  \n",
       "28   Continental shorts Wrinkle resistant travel sh...     3.168947e-02  \n",
       "439  Cap bottoms The unwavering foundation for any ...     3.196377e-02  \n",
       "399  Retro grade shorts As advantageous as a number...     3.285807e-02  \n",
       "493  Active boxer briefs A no fuss travel companion...     3.371626e-02  \n",
       "462  Custodian pants short The graveyard shift has ...     3.381079e-02  \n",
       "480  Duck pants reg Essential wear for splitting lo...     3.393221e-02  \n",
       "11   Baggies shorts Even Baggies our most popular s...     3.402412e-02  \n",
       "131  Stretch polo Core to the nomadic lifestyle our...     3.427577e-02  \n",
       "61   El cap jkt Resistant to hard play but irresist...     3.459531e-02  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.recommend(text['description'].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec2 = recommendation(text, 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.encode(use_pca=True, n_components=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sors_\\conda-gpu\\lib\\site-packages\\bert_serving\\client\\__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=40\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>cosine_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Active classic boxers - There's a reason why o...</td>\n",
       "      <td>Active classic boxers There's a reason why our...</td>\n",
       "      <td>-1.192093e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>Hip chest pack - Ready to go vest free? This i...</td>\n",
       "      <td>Hip chest pack Ready to go vest free This is t...</td>\n",
       "      <td>6.087426e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Baggies shorts - Even Baggies, our most popula...</td>\n",
       "      <td>Baggies shorts Even Baggies our most popular s...</td>\n",
       "      <td>6.328166e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>Compound cargo shorts - With cargo pockets pul...</td>\n",
       "      <td>Compound cargo shorts With cargo pockets pulle...</td>\n",
       "      <td>6.739159e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>M10 pants - Volatile climates don't rule out b...</td>\n",
       "      <td>M pants Volatile climates don't rule out big a...</td>\n",
       "      <td>6.837710e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>Synch vest - With the possible exception of du...</td>\n",
       "      <td>Synch vest With the possible exception of duct...</td>\n",
       "      <td>6.877334e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Compound cargo pants - long - The ultimate do-...</td>\n",
       "      <td>Compound cargo pants long The ultimate do ever...</td>\n",
       "      <td>7.161425e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>Marlwalker pants - Veterans of the tropics kno...</td>\n",
       "      <td>Marlwalker pants Veterans of the tropics know ...</td>\n",
       "      <td>7.177995e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>Compound cargo pants - short - The ultimate do...</td>\n",
       "      <td>Compound cargo pants short The ultimate do eve...</td>\n",
       "      <td>7.187109e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>Cap 3 bottoms - The unwavering foundation for ...</td>\n",
       "      <td>Cap bottoms The unwavering foundation for any ...</td>\n",
       "      <td>7.195204e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           description  \\\n",
       "0    Active classic boxers - There's a reason why o...   \n",
       "54   Hip chest pack - Ready to go vest free? This i...   \n",
       "11   Baggies shorts - Even Baggies, our most popula...   \n",
       "452  Compound cargo shorts - With cargo pockets pul...   \n",
       "85   M10 pants - Volatile climates don't rule out b...   \n",
       "229  Synch vest - With the possible exception of du...   \n",
       "26   Compound cargo pants - long - The ultimate do-...   \n",
       "398  Marlwalker pants - Veterans of the tropics kno...   \n",
       "451  Compound cargo pants - short - The ultimate do...   \n",
       "439  Cap 3 bottoms - The unwavering foundation for ...   \n",
       "\n",
       "                                            clean_text  cosine_distance  \n",
       "0    Active classic boxers There's a reason why our...    -1.192093e-07  \n",
       "54   Hip chest pack Ready to go vest free This is t...     6.087426e-01  \n",
       "11   Baggies shorts Even Baggies our most popular s...     6.328166e-01  \n",
       "452  Compound cargo shorts With cargo pockets pulle...     6.739159e-01  \n",
       "85   M pants Volatile climates don't rule out big a...     6.837710e-01  \n",
       "229  Synch vest With the possible exception of duct...     6.877334e-01  \n",
       "26   Compound cargo pants long The ultimate do ever...     7.161425e-01  \n",
       "398  Marlwalker pants Veterans of the tropics know ...     7.177995e-01  \n",
       "451  Compound cargo pants short The ultimate do eve...     7.187109e-01  \n",
       "439  Cap bottoms The unwavering foundation for any ...     7.195204e-01  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.recommend(text['description'].loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yeah, that is basically it. This is the first run, plenty of parameters to tune, models to train, and metrics to take care of to get better results. \n",
    "\n",
    "Thank you\n",
    "\n",
    "Your friendly neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
